{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLpractice1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Z3f_sttGtY"
      },
      "source": [
        "\n",
        "#1.\n",
        "\n",
        "Use information gain to build a decision tree that predicts the value of the class $Play$ based on the input features $Ace$, $Ten$, and $FirstMove$, using the training data provided below. Show each step of your calculations.\n",
        "\n",
        "Ace | Ten | FirstMove | Play\n",
        "--- | --- | --- | ---\n",
        "false | false | false | stand\n",
        "true | false | true | hit\n",
        "true | true | false | hit\n",
        "true | true | true | stand\n",
        "\n",
        "Is this tree optimal? In other words, does it yield zero classification error on the training data with minimal depth? Explain why or why not. If it is not optimal, draw the optimal tree as well. You can either include the tree(s) as a picture or describe using text.\n",
        "\n",
        "---\n",
        "\n",
        "Answer: The entropy of the original dataset, $S$, is Entropy(2,2) = 1.0.\n",
        "\n",
        "Gain(Ace) = Entropy(S) - [3/4Entropy($S_{Ace=true}$) + 1/4Entropy($S_{Ace=false}$)] = 1.0 - [3/4*0.918 + 1/4*0.0] = 0.311\n",
        "\n",
        "Gain(Ten) = Entropy(S) - [1/2Entropy($S_{Ten=true}$) + 1/2Entropy($S_{Ten=false}$)] = 1.0 - [1/2*1 + 1/2*1] = 0.0\n",
        "\n",
        "Gain(FirstMove) = Entropy(S) - [1/2Entropy($S_{FirstMove=true}$) + 1/2Entropy($S_{FirstMove=false}$)] = 1.0 - [1/2*1 + 1/2*1] = 0.0\n",
        "\n",
        "Thus, $Ace$ is the best choice for the root note. When Ace=false the class value for the training data is always $stand$, so this becomes a leaf node in the tree.\n",
        "\n",
        "Next, we need to select between features $Ten$ and $FirstMove$ to serve as the root node for the subset of training data when $Ace=true$. The entropy of this subset, which we will call $S1$, is Entropy(2 1) = 0.918.\n",
        "\n",
        "Gain(Ten) = Entropy(S1) - [2/3Entropy($S1_{Ten=true}$) + 1/3Entropy($S1_{Ten=false}$) = 0.918 - [2/3*1.0 + 1/3*0.0] = 0.252\n",
        "\n",
        "Gain(FirstMove) = Entropy(S1) - [2/3Entropy($S1_{FirstMove=true}$) + 1/3Entropy($S1_{FirstMove=false}$) = 0.918 - [2/3*1.0 + 1/3*0.0] = 0.252\n",
        "\n",
        "These features have equal gain. We randomly choose $Ten$ as the next feature in the tree. The final tree is thus\n",
        "\n",
        "            Ace\n",
        "     false /   \\ true\n",
        "        stand  Ten\n",
        "        false /   \\ true\n",
        "             hit FirstMove\n",
        "          false /   \\ true\n",
        "               hit  stand\n",
        "\n",
        "\n",
        "Although our greedy method ensured that the root, $Ace$, yields the best information gain, it does not yield the minimum-depth tree with 0 error. The value of the target (class) variable $Play$ is actually just the exclusive or of $Ten$ and $FirstMove$. Thus, we can build a tree of depth 2 that yields 0 error.\n",
        "\n",
        "                      Ten\n",
        "                   f /   \\ t\n",
        "             FirstMove   FirstMove\n",
        "            f /   \\ t   f /   \\ t\n",
        "            stand hit   hit   stand\n",
        "---\n",
        "\n",
        "\n",
        "#2.\n",
        "\n",
        "Express the concept (Play=Hit) learned by all of your trees in Problem 1 as logical if-then rules.\n",
        "\n",
        "---\n",
        "\n",
        "Answer: Play=Hit (tree 1): Ace=true and Ten=true and FirstMove=true\n",
        "\n",
        "Play=Hit (tree 2): (Ten=false and FirstMove=true) or (Ten=true and FirstMove=false)\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "The decision tree algorithm we discussed in class is a greedy algorithm. What does this\n",
        "mean? What is the reason that most decision tree learning algorithms are greedy?\n",
        "\n",
        "---\n",
        "\n",
        "Answer: A greedy algorithm makes a locally-optimal choice at each decision point, without considering the long-term effects (impact on achieving a globally-optimal solution). The decision tree algorithm we discussed in class selects a feature to place at the next node in the tree (building top down) without considering the impact on the later structure of the tree. Finding a globally-optimum tree (smallest tree with zero error on the training data) requires a number of computations that is exponential in the size of the problem. A greedy algorithm, in contrast, requires a number of computations that is a polynomial function of the number of data points and number of features. We use greedy decision tree algorithms to enable them to scale to large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "Suppose you are testing a new algorithm on a data set consisting of 100 positive and 100 negative examples. You plan to use leave-one-out cross-validation and compare your algorithm to a baseline function, a simple majority classifier. With leave-one-out cross-validation, you train the algorithm on 199 data points and test it on 1 data point. You repeat the process 400 times, letting each point having a chance to represent the test set, and report the average of the classification accuracies. Given a set of training data, the majority classifier always outputs the class that is in the majority in the training set, regardless of the input. You expect the majority classifier to achieve about 50% classification accuracy, but to your surprise, it scores zero every time. Why?\n",
        "\n",
        "---\n",
        "\n",
        "Answer: As soon as you remove an example for testing, it becomes the minority\n",
        "class and the majority classifier will predict the opposite class.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#5.\n",
        "\n",
        "In this problem you are asked to write a Python program using Google Colab. We provide code below to construct a decision tree using the measures of entropy and gain discussed in class. You need to enhance this program in three ways.\n",
        "\n",
        "- First, note that the provided code assumes that the features are all discrete (there is a finite number of possible feature values) and not continuous. Modify the code to handle either discrete or continuous-valued features. In this case, we ask that you convert continuous-valued features to discrete features using equal-frequency binning, where the number of bins can be a parameter or hard-coded.\n",
        "\n",
        "- Second, use the dataset found at http://eecs.wsu.edu/~cook/ml/alldata.csv to test your model. This a comma-separated data file, one line per data point. The last entry on each line is the class value and the remaining entries represent the feature values. This data is a ``human activity recognition using smartphones'' dataset. The features represent statistical summaries of sensor data collected with a phone-based 3D accelerometer (measuring phone acceleration in X, Y, and Z directions) and a 3D gyroscope (measuring 3-axial angular velocity). The phone was worn by participants while they performed two activities: sit (0) and stand (1). You will need to read in the dataset and store the data in a structure that the program can process. To test the model, randomly select a subset of points for training the model and randomly select a second subset of points for testing the model. For now, do not worry about whether the subsets overlap or not (in the future, not overlapping will be an important issue).\n",
        "\n",
        "- Third, generate a learning curve to show how the model performs on the data. You can use functions available from the matplotlib library (https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.plot.html) to generate the plot. The y axis of the plot should be classification accuracy, tested on a randomly-selected subset of 100 instances. The x axis of the plot should be the number of instances that was used for training. Create a minimum of 10 points for your learning curve plot with a broad range of x values (you can add more than 10 points to make the curve more complete).\n",
        "\n",
        "Finally, you may notice that the curve jumps around quite a bit. Smooth the curve by repeating the process at least 10 times and plotting the average of the results over the 10 trials.\n",
        "\n",
        "What insights does the learning curve provide on the learning process and the need for a large amount of training data?\n",
        "\n",
        "*Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc8fTGIkJcim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778d760d-584f-48de-c76e-11a985f57200"
      },
      "source": [
        "# Decision tree learning\n",
        "#\n",
        "# Assumes discrete features. Examples may be inconsistent. Stopping condition for tree\n",
        "# generation is when all examples have the same class, or there are no more features\n",
        "# to split on (in which case, use the majority class). If a split yields no examples\n",
        "# for a particular feature value, then the classification is based on the parent's\n",
        "# majority class.\n",
        "\n",
        "import math\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, majClass):\n",
        "        self.split_feature = -1 # -1 indicates leaf node\n",
        "        self.children = {} # dictionary of {feature_value: child_tree_node}\n",
        "        self.majority_class = majClass\n",
        "        \n",
        "def build_tree(examples):\n",
        "    if not examples:\n",
        "        return None\n",
        "    # collect sets of values for each feature index, based on the examples\n",
        "    features = {}\n",
        "    for feature_index in range(len(examples[0]) - 1):\n",
        "        features[feature_index] = set([example[feature_index] for example in examples])\n",
        "    return build_tree_1(examples, features)\n",
        "    \n",
        "def build_tree_1(examples, features):\n",
        "    tree_node = TreeNode(majority_class(examples))\n",
        "    # if examples all have same class, then return leaf node predicting this class\n",
        "    if same_class(examples):\n",
        "        return tree_node\n",
        "    # if no more features to split on, then return leaf node predicting majority class\n",
        "    if not features:\n",
        "        return tree_node\n",
        "    # split on best feature and recursively generate children\n",
        "    best_feature_index = best_feature(features, examples)\n",
        "    tree_node.split_feature = best_feature_index\n",
        "    remaining_features = features.copy()\n",
        "    remaining_features.pop(best_feature_index)\n",
        "    for feature_value in features[best_feature_index]:\n",
        "        split_examples = filter_examples(examples, best_feature_index, feature_value)\n",
        "        tree_node.children[feature_value] = build_tree_1(split_examples, remaining_features)\n",
        "    return tree_node\n",
        "\n",
        "def majority_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return max(set(classes), key = classes.count)\n",
        "\n",
        "def same_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return (len(set(classes)) == 1)\n",
        "\n",
        "def best_feature(features, examples):\n",
        "    # Return index of feature with lowest entropy after split\n",
        "    best_feature_index = -1\n",
        "    best_entropy = 2.0 # max entropy = 1.0\n",
        "    for feature_index in features:\n",
        "        se = split_entropy(feature_index, features, examples)\n",
        "        if se < best_entropy:\n",
        "            best_entropy = se\n",
        "            best_feature_index = feature_index\n",
        "    return best_feature_index\n",
        "\n",
        "def split_entropy(feature_index, features, examples):\n",
        "    # Return weighted sum of entropy of each subset of examples by feature value.\n",
        "    se = 0.0\n",
        "    for feature_value in features[feature_index]:\n",
        "        split_examples = filter_examples(examples, feature_index, feature_value)\n",
        "        se += (float(len(split_examples)) / float(len(examples))) * entropy(split_examples)\n",
        "    return se\n",
        "\n",
        "def entropy(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    classes_set = set(classes)\n",
        "    class_counts = [classes.count(c) for c in classes_set]\n",
        "    e = 0.0\n",
        "    class_sum = sum(class_counts)\n",
        "    for class_count in class_counts:\n",
        "        if class_count > 0:\n",
        "            class_frac = float(class_count) / float(class_sum)\n",
        "            e += (-1.0)* class_frac * math.log(class_frac, 2.0)\n",
        "    return e\n",
        "\n",
        "def filter_examples(examples, feature_index, feature_value):\n",
        "    # Return subset of examples with given value for given feature index.\n",
        "    return list(filter(lambda example: example[feature_index] == feature_value, examples))\n",
        "\n",
        "def print_tree(tree_node, feature_names, depth = 1):\n",
        "    indent_space = depth * \"  \"\n",
        "    if tree_node.split_feature == -1: # leaf node\n",
        "        print(indent_space + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "    else:\n",
        "        for feature_value in tree_node.children:\n",
        "            print(indent_space + feature_names[tree_node.split_feature] + \" == \" + feature_value)\n",
        "            child_node = tree_node.children[feature_value]\n",
        "            if child_node:\n",
        "                print_tree(child_node, feature_names, depth+1)\n",
        "            else:\n",
        "                # no child node for this value, so use majority class of parent (tree_node)\n",
        "                print(indent_space + \"  \" + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "\n",
        "def classify(tree_node, instance):\n",
        "    if tree_node.split_feature == -1:\n",
        "        return tree_node.majority_class\n",
        "    child_node = tree_node.children[instance[tree_node.split_feature]]\n",
        "    if child_node:\n",
        "        return classify(child_node, instance)\n",
        "    else:\n",
        "        return tree_node.majority_class\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   feature_names = [\"Color\", \"Type\", \"Origin\", \"Stolen\"]\n",
        "   \n",
        "   examples = [\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"Yes\"],\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"No\"],\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"Yes\"],\n",
        "       [\"Yellow\", \"Sports\", \"Domestic\", \"No\"],\n",
        "       [\"Yellow\", \"Sports\", \"Imported\", \"Yes\"],\n",
        "       [\"Yellow\", \"SUV\", \"Imported\", \"No\"],\n",
        "       [\"Yellow\", \"SUV\", \"Imported\", \"Yes\"],\n",
        "       [\"Yellow\", \"SUV\", \"Domestic\", \"No\"],\n",
        "       [\"Red\", \"SUV\", \"Imported\", \"No\"],\n",
        "       [\"Red\", \"Sports\", \"Imported\", \"Yes\"]\n",
        "       ]\n",
        "   tree = build_tree(examples)\n",
        "   print(\"Tree:\")\n",
        "   print_tree(tree, feature_names)\n",
        "   test_instance = [\"Red\", \"SUV\", \"Domestic\"]\n",
        "   test_class = classify(tree, test_instance)\n",
        "   print(\"\\nTest instance: \" + str(test_instance))\n",
        "   print(\"  class = \" + test_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tree:\n",
            "  Type == SUV\n",
            "    Color == Red\n",
            "      Stolen: No\n",
            "    Color == Yellow\n",
            "      Origin == Domestic\n",
            "        Stolen: No\n",
            "      Origin == Imported\n",
            "        Stolen: Yes\n",
            "  Type == Sports\n",
            "    Origin == Domestic\n",
            "      Color == Red\n",
            "        Stolen: Yes\n",
            "      Color == Yellow\n",
            "        Stolen: No\n",
            "    Origin == Imported\n",
            "      Stolen: Yes\n",
            "\n",
            "Test instance: ['Red', 'SUV', 'Domestic']\n",
            "  class = No\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca0MT5FJWVd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5e20d58f-bbf2-431b-9a8e-812f86c837bb"
      },
      "source": [
        "# Decision tree learning - Solution\n",
        "#\n",
        "# Handles discrete or continuous features, specified by global variable continuous.\n",
        "# Continuous-valued variables are discretized into num_bins bins using\n",
        "# equal-frequency binning. The stopping condition for tree\n",
        "# generation is when all examples have the same class, or there are no more features\n",
        "# to split on (in which case, use the majority class). If a split yields no examples\n",
        "# for a particular feature value, then the classification is based on the parent's\n",
        "# majority class.\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from random import sample\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# global variables\n",
        "num_bins = 2\n",
        "continuous = True\n",
        "num_trials = 10\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, majClass):\n",
        "        self.split_feature = -1 # -1 indicates leaf node\n",
        "        self.children = {} # dictionary of {feature_value: child_tree_node}\n",
        "        self.majority_class = majClass\n",
        "        \n",
        "def build_tree(examples):\n",
        "    if not examples:\n",
        "        return None\n",
        "    # collect sets of values for each feature index, based on the examples\n",
        "    features = {}\n",
        "    for feature_index in range(len(examples[0]) - 1):\n",
        "        features[feature_index] = set([example[feature_index] for example in examples])\n",
        "    return build_tree_1(examples, features)\n",
        "    \n",
        "def build_tree_1(examples, features):\n",
        "    tree_node = TreeNode(majority_class(examples))\n",
        "    # if examples all have same class, then return leaf node predicting this class\n",
        "    if same_class(examples):\n",
        "        return tree_node\n",
        "    # if no more features to split on, then return leaf node predicting majority class\n",
        "    if not features:\n",
        "        return tree_node\n",
        "    # split on best feature and recursively generate children\n",
        "    best_feature_index = best_feature(features, examples)\n",
        "    tree_node.split_feature = best_feature_index\n",
        "    remaining_features = features.copy()\n",
        "    remaining_features.pop(best_feature_index)\n",
        "    for feature_value in features[best_feature_index]:\n",
        "        split_examples = filter_examples(examples, best_feature_index, feature_value)\n",
        "        tree_node.children[feature_value] = build_tree_1(split_examples, remaining_features)\n",
        "    return tree_node\n",
        "\n",
        "def majority_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return max(set(classes), key = classes.count)\n",
        "\n",
        "def same_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return (len(set(classes)) == 1)\n",
        "\n",
        "def best_feature(features, examples):\n",
        "    # Return index of feature with lowest entropy after split\n",
        "    best_feature_index = -1\n",
        "    best_entropy = 2.0 # max entropy = 1.0\n",
        "    for feature_index in features:\n",
        "        se = split_entropy(feature_index, features, examples)\n",
        "        if se < best_entropy:\n",
        "            best_entropy = se\n",
        "            best_feature_index = feature_index\n",
        "    return best_feature_index\n",
        "\n",
        "def split_entropy(feature_index, features, examples):\n",
        "    # Return weighted sum of entropy of each subset of examples by feature value.\n",
        "    se = 0.0\n",
        "    for feature_value in features[feature_index]:\n",
        "        split_examples = filter_examples(examples, feature_index, feature_value)\n",
        "        se += (float(len(split_examples)) / float(len(examples))) * entropy(split_examples)\n",
        "    return se\n",
        "\n",
        "def entropy(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    classes_set = set(classes)\n",
        "    class_counts = [classes.count(c) for c in classes_set]\n",
        "    e = 0.0\n",
        "    class_sum = sum(class_counts)\n",
        "    for class_count in class_counts:\n",
        "        if class_count > 0:\n",
        "            class_frac = float(class_count) / float(class_sum)\n",
        "            e += (-1.0)* class_frac * math.log(class_frac, 2.0)\n",
        "    return e\n",
        "\n",
        "def filter_examples(examples, feature_index, feature_value):\n",
        "    # Return subset of examples with given value for given feature index.\n",
        "    return list(filter(lambda example: example[feature_index] == feature_value, examples))\n",
        "\n",
        "def print_tree(tree_node, feature_names, depth = 1):\n",
        "    indent_space = depth * \"  \"\n",
        "    if tree_node.split_feature == -1: # leaf node\n",
        "        print(indent_space + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "    else:\n",
        "        for feature_value in tree_node.children:\n",
        "            print(indent_space + feature_names[tree_node.split_feature] + \" == \" + feature_value)\n",
        "            child_node = tree_node.children[feature_value]\n",
        "            if child_node:\n",
        "                print_tree(child_node, feature_names, depth+1)\n",
        "            else:\n",
        "                # no child node for this value, so use majority class of parent (tree_node)\n",
        "                print(indent_space + \"  \" + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "\n",
        "def classify(tree_node, instance):\n",
        "    if tree_node.split_feature == -1:\n",
        "        return tree_node.majority_class\n",
        "    child_node = tree_node.children[instance[tree_node.split_feature]]\n",
        "    if child_node:\n",
        "        return classify(child_node, instance)\n",
        "    else:\n",
        "        return tree_node.majority_class\n",
        "\n",
        "def discretize(data):\n",
        "  n1 = len(data)\n",
        "  n2 = n1 / 2\n",
        "  values = copy.copy(data)\n",
        "  values.sort()\n",
        "  threshold = values[n2]\n",
        "  newdata = np.zeros(n1, dtype=str)\n",
        "  for i in range(n1):\n",
        "    if data[i] <= threshold:\n",
        "      newdata[i] = 'small'\n",
        "    else:\n",
        "      newdata[i] = 'large'\n",
        "  return newdata\n",
        "\n",
        "def plot_results(x, y):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x, y)\n",
        "  ax.set(xlabel='training size', ylabel='classification accuracy',\n",
        "         title='Learning curve')\n",
        "  ax.grid()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   random.seed()\n",
        "   df = pd.read_csv('https://eecs.wsu.edu/~cook/ml/alldata.csv')\n",
        "   data = df.to_numpy()\n",
        "   d = len(data[0])\n",
        "   n = len(data)\n",
        "   if continuous:\n",
        "      examples = np.zeros(np.shape(data), dtype=str)\n",
        "      for i in range(d-1):\n",
        "         examples[:,i] = discretize(data[:,i])\n",
        "   else:\n",
        "      examples = data\n",
        "   for i in range(n):\n",
        "     examples[i][d-1] = str(data[i][d-1])\n",
        "   \"\"\"\n",
        "   feature_names = [\"Color\", \"Type\", \"Origin\", \"Stolen\"]\n",
        "   \n",
        "   examples = [\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"Yes\"],\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"No\"],\n",
        "       [\"Red\", \"Sports\", \"Domestic\", \"Yes\"],\n",
        "       [\"Yellow\", \"Sports\", \"Domestic\", \"No\"],\n",
        "       [\"Yellow\", \"Sports\", \"Imported\", \"Yes\"],\n",
        "       [\"Yellow\", \"SUV\", \"Imported\", \"No\"],\n",
        "       [\"Yellow\", \"SUV\", \"Imported\", \"Yes\"],\n",
        "       [\"Yellow\", \"SUV\", \"Domestic\", \"No\"],\n",
        "       [\"Red\", \"SUV\", \"Imported\", \"No\"],\n",
        "       [\"Red\", \"Sports\", \"Imported\", \"Yes\"]\n",
        "       ]\n",
        "   tree = build_tree(examples)\n",
        "   print(\"Tree:\")\n",
        "   print_tree(tree, feature_names)\n",
        "   test_instance = [\"Red\", \"SUV\", \"Domestic\"]\n",
        "   test_class = classify(tree, test_instance)\n",
        "   print(\"\\nTest instance: \" + str(test_instance))\n",
        "   print(\"  class = \" + test_class)\n",
        "   \"\"\"\n",
        "   samples = []\n",
        "   results = []\n",
        "   for trials in range(num_trials):\n",
        "      count = 0\n",
        "      for i in range(1, 200, 5):\n",
        "         test_set = sample(examples, 100)\n",
        "         train_set = sample(examples, i)\n",
        "         tree = build_tree(train_set)\n",
        "         right = 0\n",
        "         for j in range(100):\n",
        "            test_class = classify(tree, test_set[j])\n",
        "            if test_class == test_set[j][d-1]:\n",
        "               right += 1\n",
        "         if trials == 0:\n",
        "            samples.append(i)\n",
        "            results.append(right)\n",
        "         else:\n",
        "            results[count] += right\n",
        "         count += 1\n",
        "   for i in range(len(results)):\n",
        "      results[i] = float(results[i]) / float(num_trials)\n",
        "   plot_results(samples, results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}