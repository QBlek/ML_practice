{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLpractice5",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QBlek/ML_practice/blob/main/MLpractice5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1vAj5wklz1"
      },
      "source": [
        "#1.\n",
        "\n",
        "In most real world scenarios, data contain outliers. When using a support vector machine, outliers can be dealt with using a soft margin, specified in a slightly different optimization problem shown in Equation 7.38 in the text and called a soft-margin SVM.\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i = 0$? Is this data point classified correctly?\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $0 < \\zeta_i \\leq 1$? Is this data point classified correctly?\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i > 1$? Is this data point classified correctly?\n",
        "\n",
        "---\n",
        "\n",
        "Answer: This answer focuses on positive examples (the same argument applies to negative examples where $y_i = -1$). Here, the slack $\\zeta_i$ for example $i$ represents the distance from the example to the positive (upper-right) margin.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1YtpoLHCCxWlL7BpUR13Oc0R0ZmZdcmlv)\n",
        "\n",
        "For $\\zeta_i < 0$, the example is well outside the margin and correctly classified as a positive example. The SVM constrains all $\\zeta_i \\geq 0$, so this $\\zeta_i$ would be set to zero in the minimization equation (7.38).\n",
        "\n",
        "For $\\zeta_i = 0$, the example is on the margin and correctly classified as a positive example. \n",
        "\n",
        "For $0 < \\zeta_i < 1$, the example is inside the margin, but still correctly classified as a positive example.\n",
        "\n",
        "For $\\zeta_i = 1$, the example is right on the hyperplane. Typically points on the hyperplane are classified as positive, but it could go either way.\n",
        "\n",
        "For $\\zeta_i > 1$, the example is on the wrong side of the hyperplane, so it will be incorrectly classified as a negative example.\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "Suppose the two-layer neural network shown below processes the input (0, 1, 1, 0). If the actual output should be 0.2, show step-by-step how the vector of weights *v* will be updated using backpropagation and $\\eta = 0.2$.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1mLkFgXA0drWp6nYL50n0BZv2Z13EA9CN)\n",
        "\n",
        "---\n",
        "\n",
        "Answer: The activation of the hidden units is calculated using the formula $a_i = w_i . x$, so the activation for hidden units 1, 2, and 3 (numbered top to bottom) is $tanh(1.0 \\times 0 + 1.0 \\times 1 + 1.0 \\times 1 + 1.0 \\times 0) = tanh(2.0) = 0.964$.\n",
        "\n",
        "The output of the network is next calculated using the formula $v . h$, which equals $-0.4 \\times 0.964 + 1.0 \\times 0.964 + 0.2 \\times 0.964 =  0.7712$. The error $e = 0.2 - 0.7712 = -0.5712$. The vector of weights *v* is updated using the formula $v = v - \\eta g$ where $g = g - eh$. Here $g = 0.0 + 0.5712 \\times 0.964 = 0.5506$ for the output layer and $v_1 = -0.4 - 0.2 \\times 0.5506 = -0.5101$, $v_2 = 1.0 - 0.2 \\times 0.5506 = 0.8899$, and $v_3 = 0.2 - 0.2 \\times 0.5506 = 0.0899$.\n",
        "\n",
        "To completely update the network we would next need to update the vector of weights $W$ using information that includes the error of the network, the previous $W$, the learning rate, and the output of the hidden node to which the edge points.\n",
        "\n",
        "---\n",
        "\n",
        "#3. \n",
        "\n",
        "Under which of these conditions does an ensemble classifier perform best? There can be more than one right answer, explain all of your responses.\n",
        "\n",
        "- Low prediction correlation between base classifiers.\n",
        "- High prediction correlation between base classifiers.\n",
        "- Base classifiers have low variance.\n",
        "- Base classifiers have high bias.\n",
        "- Base classifiers have high variance.\n",
        "\n",
        "---\n",
        "\n",
        "Answer: A lower correlation among base classifiers will increase the error-correcting capability of the ensemble, so this situation is preferred.\n",
        "\n",
        "Weak learners are typically used in ensemble methods. They have low variance and do not typically overfit the training data. They also typically have low bias.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "The goal of this problem is for you to implement backpropagation from scratch. You can make use of python libraries for handling the data and computation, but implement the actual activation and weight change calculations yourself.\n",
        "\n",
        "Test your neural network using the MNIST dataset. Information on loading and storing this handwritten-digit dataset can be found at https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html. Only consider digit classes '0' (which you can map onto value -1) and '1' (which you can map onto value 1). Train the network on a randomly-selected 2/3 of the data points and test on the remaining 1/3. You can report mean squared error or accuracy for the test data for a minimum of 10 epochs.\n",
        "\n",
        "Below this code I provide a separate backprop code, slightly simpler with just one output node, to classify breast cancer data. The second set of code more closely mirrors the Daume book.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFzevjgvygJQ",
        "outputId": "91f5a313-02a6-4ba9-e441-a3fd93b1f5e0"
      },
      "source": [
        "from math import exp\n",
        "from random import random\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "  network = list()  # initialize weights to random number in [0..1]\n",
        "  hidden_layer = [{'weights':[random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
        "  network.append(hidden_layer)\n",
        "  output_layer = [{'weights':[random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
        "  network.append(output_layer)\n",
        "  return network\n",
        "\n",
        "\n",
        "def activate(weights, inputs):\n",
        "  activation = weights[-1]   # bias\n",
        "  for i in range(len(weights)-1):\n",
        "    activation += weights[i] * inputs[i]\n",
        "  return activation\n",
        "\n",
        "\n",
        "def transfer(activation): # sigmoid function\n",
        "  return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "\"\"\"\n",
        "def transfer(activation): # tanh function\n",
        "  return np.tanh(activation)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def forward_propagate(network, X, y):\n",
        "  inputs = X\n",
        "  for layer in network:\n",
        "    new_inputs = []\n",
        "    for node in layer:\n",
        "      activation = activate(node['weights'], X)\n",
        "      node['output'] = transfer(activation)\n",
        "      new_inputs.append(node['output']) # output of one node input to another\n",
        "    inputs = new_inputs\n",
        "  return inputs   # return output from last layer\n",
        "\n",
        "\n",
        "def transfer_derivative(output): # derivative of sigmoid function\n",
        "  return output * (1.0 - output)\n",
        "\n",
        "\"\"\"\n",
        "def transfer_derivative(output): # derivative of tanh function\n",
        "  return 1.0 - np.tanh(output)**2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def backward_propagate_error(network, expected):\n",
        "  for i in reversed(range(len(network))): # from output back to input layers\n",
        "    layer = network[i]\n",
        "    errors = list()\n",
        "    if i != len(network)-1:  # not the output layer\n",
        "      for j in range(len(layer)):\n",
        "        error = 0.0\n",
        "        for node in network[i+1]:\n",
        "          error += (node['weights'][j] * node['delta'])\n",
        "        errors.append(error)\n",
        "    else:   # output layer\n",
        "      for j in range(len(layer)):\n",
        "        node = layer[j]\n",
        "        errors.append(expected[j] - node['output'])\n",
        "    for j in range(len(layer)):\n",
        "      network[i][j]['delta'] = errors[j] * transfer_derivative(node['output'])\n",
        "  return network\n",
        "\n",
        "\n",
        "def update_weights(network, x, eta):\n",
        "  for i in range(len(network)):\n",
        "    inputs = x\n",
        "    if i != 0:\n",
        "      inputs = [node['output'] for node in network[i-1]]\n",
        "    for n in range(len(network[i])):\n",
        "      node = network[i][n]\n",
        "      for j in range(len(inputs)):\n",
        "        network[i][n]['weights'][j] += eta * node['delta'] * inputs[j]\n",
        "      network[i][n]['weights'][-1] += eta * node['delta']\n",
        "  return network\n",
        "\n",
        "\n",
        "def train_network(network, X, y, eta, num_epochs, num_outputs):\n",
        "  expected = np.full((2), 0)\n",
        "  for epoch in range(num_epochs):\n",
        "    sum_error = 0\n",
        "    # There are two output nodes. The one corresponding to the correct label\n",
        "    # should output 1, the other should output -1.\n",
        "    for i in range(len(y)):\n",
        "      outputs = forward_propagate(network, X[i], y[i])\n",
        "      if y[i] == 0:\n",
        "        expected[0] = 1\n",
        "        expected[1] = 0\n",
        "      else:\n",
        "        expected[0] = 0\n",
        "        expected[1] = 1\n",
        "      sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
        "      network = backward_propagate_error(network, expected)\n",
        "      network = update_weights(network, X[i], eta)\n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, eta, sum_error))\n",
        "  return network\n",
        "\n",
        "\n",
        "def test_network(network, X, y, num_outputs):\n",
        "  expected = np.full((2), 0)\n",
        "  sum_error = 0\n",
        "  # There are two output nodes. The one corresponding to the correct label\n",
        "  # should output 1, the other should output -1.\n",
        "  for i in range(len(y)):\n",
        "    outputs = forward_propagate(network, X[i], y[i])\n",
        "    if y[i] == 0:\n",
        "      expected[0] = 1\n",
        "      expected[1] = 0\n",
        "    else:\n",
        "      expected[0] = 0\n",
        "      expected[1] = 1\n",
        "    sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
        "  print('mse of test data is', sum_error / float(len(y)))\n",
        "\n",
        "\n",
        "def main_mnist():\n",
        "  # Load data from https://www.openml.org/d/554\n",
        "  features, targets = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(targets)):\n",
        "    if targets[i] == '1' or targets[i] == '0':\n",
        "      X.append(features[i])\n",
        "      if targets[i] == '0':\n",
        "        y.append(0)\n",
        "      else:\n",
        "        y.append(1)\n",
        "  n_inputs = len(X[0])\n",
        "  n_outputs = 2  # possible class values are '0' and '1'\n",
        "  # Create a network with 1 hidden layer containing 2 nodes\n",
        "  network = initialize_network(n_inputs, 2, n_outputs)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, test_size=0.33)\n",
        "  # train network for 10 epochs using learning rate of 0.1 \n",
        "  network = train_network(network, X_train, y_train, 0.1, 10, n_outputs)\n",
        "  for layer in network:\n",
        "    print('layer \\n', layer)\n",
        "  test_network(network, X_test, y_test, n_outputs)\n",
        "\n",
        "def main_bc():\n",
        "  X, y = load_breast_cancer(return_X_y=True)\n",
        "  n_inputs = len(X[0])\n",
        "  n_outputs = 2  # possible class values are '0' and '1'\n",
        "  # Create a network with 1 hidden layer containing 2 nodes\n",
        "  network = initialize_network(n_inputs, 2, n_outputs)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, test_size=0.33)\n",
        "  # train network for 50 epochs using learning rate of 0.1 \n",
        "  network = train_network(network, X_train, y_train, 0.1, 10, n_outputs)\n",
        "  for layer in network:\n",
        "    print('layer \\n', layer)\n",
        "  test_network(network, X_test, y_test, n_outputs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #main_mnist()\n",
        "  main_bc()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch=0, lrate=0.100, error=380.953\n",
            ">epoch=1, lrate=0.100, error=380.943\n",
            ">epoch=2, lrate=0.100, error=380.927\n",
            ">epoch=3, lrate=0.100, error=380.896\n",
            ">epoch=4, lrate=0.100, error=380.837\n",
            ">epoch=5, lrate=0.100, error=380.714\n",
            ">epoch=6, lrate=0.100, error=380.436\n",
            ">epoch=7, lrate=0.100, error=379.758\n",
            ">epoch=8, lrate=0.100, error=377.935\n",
            ">epoch=9, lrate=0.100, error=372.557\n",
            "layer \n",
            " [{'weights': [0.6839030751490057, 0.4041748063424126, 0.9799659374311156, 0.6212718178274422, 0.10697068445709385, 0.31492660451447835, 0.20796602164147868, 0.03563923837928916, 0.31862346701535127, 0.37057017938335296, 0.709151182215734, 0.25169059795853543, 0.2830923083183624, 0.019467026791268043, 0.383262415915383, 0.13567089574731434, 0.7018507676733173, 0.3762958668792869, 0.587046885143515, 0.5906844704242051, 0.07962836212649402, 0.3714021341288188, 0.8725636654877679, 0.752166600686019, 0.5308187935440984, 0.9938019318895014, 0.43206430139074653, 0.06747437522328514, 0.061426909342788626, 0.597383031433212, 0.5834224898194188], 'output': 1.0, 'delta': -8.80710239120772e-09}, {'weights': [0.32102703528301585, 0.9564468859025259, 0.7530508667917077, 0.6263975780231588, 0.655713551994694, 0.5154851034961452, 0.43479429629465766, 0.366095487788494, 0.9529266442490577, 0.7849291555796704, 0.4763321610067512, 0.3971078968354241, 0.06771652058295147, 0.8145091547646808, 0.09040358266241005, 0.6117712555878173, 0.6868130042893354, 0.5146583682857387, 0.9411490727604425, 0.23738343499031087, 0.39583842566130123, 0.8518613078321817, 0.9535584948054032, 0.4582831245955163, 0.9206747212758581, 0.22982272644163615, 0.47833073717942093, 0.9408713622785577, 0.5324359113116096, 0.2462986519115331, 0.02174387334011693], 'output': 1.0, 'delta': -9.286684193562569e-09}]\n",
            "layer \n",
            " [{'weights': [0.2605919076541192, -0.015250121489026842, 0.6322398515546931], 'output': 0.9933861404623653, 'delta': 1.3992518006818939e-06}, {'weights': [0.1985109084604405, 0.2074031310451976, 0.6203349134106332], 'output': 0.9997883916420113, 'delta': -0.00021151881126980226}]\n",
            "mse of test data is 0.9579185673247302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyEurb-320ad"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import math\n",
        "\n",
        "def compute_activation(weights, input):\n",
        "  activation = 0\n",
        "  for i in range(len(weights)):\n",
        "    activation += weights[i] * input[i]\n",
        "  return activation\n",
        "\n",
        "def transfer(activation): # sigmoid function\n",
        "  return 1.0 / (1.0 + np.exp(-activation))\n",
        "\n",
        "def deriv_transfer(activation):\n",
        "  return activation * (1.0 - activation)\n",
        "\n",
        "def forward_propagate(W, v, X):\n",
        "  a = np.zeros((len(W)))  # hidden node values\n",
        "  h = np.zeros((len(W)))  # hidden node values\n",
        "  for i in range(len(W)):  # compute hidden node activation\n",
        "    a[i] = compute_activation(W[i], X)\n",
        "    h[i] = transfer(a[i])\n",
        "  y_predict = compute_activation(v, h)\n",
        "  return a, h, y_predict\n",
        "\n",
        "def train_network(W, v, X, y, eta, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    sum_error = 0\n",
        "    for d in range(len(y)):\n",
        "      G = np.zeros((len(W), len(X[0])))\n",
        "      g = np.zeros((len(W)))\n",
        "      a, h, y_predict = forward_propagate(W, v, X[d])\n",
        "      error = y[d] - y_predict\n",
        "      sum_error = sum_error + (error * error)\n",
        "      for i in range(len(W)):\n",
        "        g[i] = g[i] - error * h[i]\n",
        "      for i in range(len(W)):   # hidden\n",
        "        for j in range(len(X[0])):   # input\n",
        "          G[i][j] = G[i][j] - error * v[i] * deriv_transfer(a[i]) * X[d][j]\n",
        "      for i in range(len(W)):\n",
        "        v[i] = v[i] - eta * g[i]\n",
        "      for i in range(len(W)):\n",
        "        for j in range(len(W[0])):\n",
        "          W[i][j] = W[i][j] - eta * G[i][j]\n",
        "    print('epoch', epoch, 'error', sum_error / float(len(y)))\n",
        "  return W, v\n",
        "\n",
        "# This implementation is designed for one output node\n",
        "def test_network(W, v, X, y):\n",
        "  sum_error = 0\n",
        "  for i in range(len(y)):\n",
        "    _, _, y_predict = forward_propagate(W, v, X[i])\n",
        "    sum_error += (y[i] - y_predict)**2\n",
        "  print('mse =', sum_error / float(len(y)))\n",
        "\n",
        "def main_bc():\n",
        "  X, y = load_breast_cancer(return_X_y=True)\n",
        "  X = preprocessing.normalize(X, axis=0)\n",
        "  num_input = len(X[0])\n",
        "  num_hidden = 2\n",
        "  # Define a network by two layers of random weights in range (0,1]\n",
        "  W = np.random.uniform(low=-0.5, high=0.5, size=(num_hidden, num_input))\n",
        "  v = np.random.uniform(low=-0.5, high=0.5, size=(num_hidden))\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, test_size=0.33)\n",
        "  # train network for 50 epochs using learning rate of 0.1\n",
        "  W, v = train_network(W, v, X_train, y_train, 0.01, 50)\n",
        "  test_network(W, v, X_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main_bc()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}