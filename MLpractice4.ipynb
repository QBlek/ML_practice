{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLpractice4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QBlek/ML_practice/blob/main/MLpractice4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsLGOMPRM8xK"
      },
      "source": [
        "#1.\n",
        "\n",
        "Consider the subset of (x,y) points shown below. These are actually a subset of the data points found in the sklearn diabetes dataset.\n",
        "\n",
        "x | y\n",
        "--- | ---\n",
        " 0.08 | 233\n",
        "-0.04 | 91\n",
        " 0.01 | 111\n",
        "-0.04 | 152\n",
        "-0.03 | 120\n",
        " 0.01 | 67\n",
        " 0.09 | 310\n",
        "-0.03 | 94\n",
        "-0.06 | 183\n",
        "-0.03 | 66\n",
        " 0.06 | 173\n",
        "-0.06 | 72\n",
        " 0.00 | 49\n",
        "-0.02 | 64\n",
        "-0.07 | 48\n",
        "\n",
        "\n",
        "For a candidate linear regressor with parameters $\\Theta_0 = 75.1$ and $\\Theta_1 = -0.001$, calculate the mean squared error with respect to the data points and perform one iteration of gradient descent , assuming $\\alpha = 0.01$. Show all of your work.\n",
        "\n",
        "---\n",
        "\n",
        "Answer: The mean squared error is 7524.174. $\\Theta_0 $ is updated by first computing the sum of errors, multiplying this value by $\\alpha$, and adding the result to the previous value, resulting in $\\Theta_0$ = 82.165.\n",
        "\n",
        "$\\Theta_1$ is udpated by computing the sum of each error multiplied by the corresponding value of x, multiplying the sum by $\\alpha$, and adding the result to the previous value, resulting in $\\Theta_1$ = 0.312. The result does move the regression closer to an optimal result as indicated by the new mean squared error of 6907.257.\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "Explain what value is generated by the equation $h(x) = \\frac{1}{1+e^{-(\\Theta_0 + \\Theta_1 x)}}$ in logistic regression. What steps are taken to convert the linear regression algorithm into the classification-based logistic regression algorithm?\n",
        "\n",
        "---\n",
        "\n",
        "Answer: $h(x)$ represents the probability that $x$ belongs to the positive class. This value is typically compared to a threshold to determine whether to output $+$ or $-$.\n",
        "\n",
        "Linear linear regression, logistic regression learns parameters $\\Theta_0$ and $\\Theta_1$. Linear regression learns a line but this is not effective for classification when the line dips below 0 or rises about 1. For this reason, we modify the decision boundary to flatten out when it hits $y=0$ and $y=1$ (a sigmoid function). Unlike linear regression, the output represents a probability that $y=1$ rather than a continuous value of $y$. Additionally, we have to modify the linear regression cost function to make it context so we can apply gradient descent. The cost function for linear regression is the sum of squared error. Logistic regression uses logistic loss, or Cost(h(x), y) = -y log(h(x)) - ((-y) log (1-h(x)).\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "Regularization is introduced in class as a numeric term that can be incorporated into a loss function. This is straightforward for algorithms such as neural networks that seek to optimize a numeric loss function. Here, explain possible ways regularization can be used to fine tune other types of algorithms, specifically decision trees and naive Bayes classifiers.\n",
        "\n",
        "---\n",
        "\n",
        "Answer: For decision trees, regularization takes the form of pruning the learned tree. Aggressive pruning will result in underfitting the data, while little to no pruning may result in overfitting the data.\n",
        "\n",
        "Typically, naive Bayes always includes all features in its calculation of the most likely output. $L_1$ regularization offers a valuable variable selecction property that we can use to prune irrelevant and/or redundant predictors.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "The goal of this program is to give you detailed experience with naive Bayes classifiers as well as with text mining methods and libraries. In this program, you are tasked with writing a naive Bayes classifier to classifier phone messages as spam versus not spam (ham). To test your program, use the labeled data available from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. The data is also available at https://drive.google.com/file/d/1nn2baOauApGbxrOeTb4l-30Qz1prPIS3/view?usp=sharing.\n",
        "\n",
        "You will need to read in each message and convert it to a set of features. Use the sklearn libraries to\n",
        "\n",
        "- remove punctuation\n",
        "- convert to lower case\n",
        "- create a bag of words vector that stores term frequency\n",
        "- filter out words from the stop list (stop_words)\n",
        "- include 1-grams and 2-grams\n",
        "- normalize frequences based on document length (tfidf)\n",
        "\n",
        "Report performance of your classifier on 3-fold cross validation in terms of accuracy and macro f1 score.\n",
        "\n",
        "Additionally, notice that the class distribution is imbalanced. There are 4,827 legitimate messages and 747 spam messages. Experiment with three alternative methods for addressing this issue and report impact of each method on performance.\n",
        "\n",
        "- Undersample the majority class so they are balanced.\n",
        "- Oversample the minority class so they are balanced.\n",
        "- Weight the data points based on class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okBsH6qzAg6K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "085d8e2c-198e-47e7-aba8-bbfa44ea3c16"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import string\n",
        "import copy\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "def read_data():\n",
        "  infile = open('/content/gdrive/My Drive/ML/HW/SMSSpamCollection')\n",
        "  data = []\n",
        "  y = []\n",
        "  for line in infile:\n",
        "    vector = line.strip().lower().split('\\t')\n",
        "    data.append(vector[1])\n",
        "    y.append(vector[0])\n",
        "  return data, y\n",
        "\n",
        "\n",
        "def learn(X, y, weights):\n",
        "  # train a naive Bayes classifier on data\n",
        "  clf = MultinomialNB().fit(X, y, sample_weight=weights)\n",
        "  scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
        "  print('Accuracy over 3 folds:', np.mean(scores))\n",
        "  scores = cross_val_score(clf, X, y, cv=3, scoring='f1_macro')\n",
        "  print('Macro f1 score over 3 folds:', np.mean(scores))\n",
        "\n",
        "\n",
        "def main():\n",
        "  data, y = read_data()\n",
        "  count_vect = CountVectorizer()\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  count_vect.set_params(tokenizer=tokenizer.tokenize)\n",
        "\n",
        "  # include 1-grams and 2-grams\n",
        "  count_vect.set_params(ngram_range=(1,2))\n",
        "\n",
        "  X_counts = count_vect.fit_transform(data)\n",
        "\n",
        "  # normalize counts based on document length\n",
        "  # weight common words less (is, a, an, the)\n",
        "  tfidf_transformer = TfidfTransformer()\n",
        "  X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "  # learn with equal weights, no sampling\n",
        "  print('unweighted')\n",
        "  weights = np.full(len(y), 1.0, dtype=float)\n",
        "  learn(X_tfidf, y, weights)\n",
        "\n",
        "  # learn with weights inversely proportional to class size\n",
        "  weights = compute_sample_weight(\"balanced\", y)\n",
        "  print('weights', weights)\n",
        "  learn(X_tfidf, y, weights)\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "unweighted\n",
            "Accuracy over 3 folds: 0.923394330821672\n",
            "Macro f1 score over 3 folds: 0.7786465483571515\n",
            "weights [0.57737725 0.57737725 3.73092369 ... 0.57737725 0.57737725 0.57737725]\n",
            "Accuracy over 3 folds: 0.923394330821672\n",
            "Macro f1 score over 3 folds: 0.7786465483571515\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}